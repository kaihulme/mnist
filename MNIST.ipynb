{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "(X_train_data, y_train_data), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(X_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation set and normalise pixel intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train_data, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42)\n",
    "\n",
    "X_train, X_val, X_test = (X_train/255.0), (X_val/255.0), (X_test/255.0)\n",
    "\n",
    "num_classes = 10\n",
    "rows, cols = X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Sequential([\n",
    "             Flatten(input_shape=[rows, cols]),\n",
    "             Dense(300, activation='relu'),\n",
    "             Dense(100, activation='relu'),\n",
    "             Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_trained_model = base_model.fit(X_train, y_train, \n",
    "                                    epochs=10,\n",
    "                                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot validation accuracy across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_trained_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(base_trained_model.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks and Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_model = Sequential([\n",
    "                 Flatten(input_shape=[rows, cols]),\n",
    "                 Dense(300, activation='relu'),\n",
    "                 Dense(100, activation='relu'),\n",
    "                 Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log file for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(logdir):\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(logdir, run_id) \n",
    "\n",
    "callback_root_logdir = os.path.join(os.curdir, \"callback_logs\")\n",
    "callback_run_logdir = get_run_logdir(callback_root_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks for model saving, early stopping and TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(\"callback_model.h5\"),\n",
    "             EarlyStopping(patience=10, restore_best_weights=True),\n",
    "             TensorBoard(callback_run_logdir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model with large number of epochs to allow for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback_trained_model = callback_model.fit(X_train, y_train, \n",
    "                                            epochs=100,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run TensorBoard session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./callback_logs --port=6008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback class for increasing learning rate by a given factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "class IncreasingLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlr_model = Sequential([\n",
    "            Flatten(input_shape=[rows, cols]),\n",
    "            Dense(300, activation='relu'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model starting at 1e-3 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlr_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=SGD(lr=1e-3), \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlr_root_logdir = os.path.join(os.curdir, \"vlr_logs\")\n",
    "vlr_run_logdir = get_run_logdir(vlr_root_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks including increase learning rate by 0.5% each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(\"vlr_model.h5\"),\n",
    "             EarlyStopping(patience=10, restore_best_weights=True),\n",
    "             TensorBoard(callback_run_logdir),\n",
    "             IncreasingLearningRate(factor=1.005)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model with increasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1500 [..............................] - ETA: 0s - loss: 2.2971 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0439s). Check your callbacks.\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: nan - accuracy: 0.6787 - val_loss: nan - val_accuracy: 0.0979\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: nan - accuracy: 0.0989 - val_loss: nan - val_accuracy: 0.0979\n",
      "Epoch 3/100\n",
      " 357/1500 [======>.......................] - ETA: 5s - loss: nan - accuracy: 0.0969"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6557d661f117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vlr_trained_model = vlr_model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   callbacks=callbacks)\n",
      "\u001b[0;32m~/Git/mnist/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/mnist/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/mnist/.env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mstep_increment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep_increment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;34m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vlr_trained_model = vlr_model.fit(X_train, y_train,\n",
    "                                  epochs=100,\n",
    "                                  validation_data=(X_val, y_val),\n",
    "                                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isnt working but the idea is to plot learning rate against loss to find an optimal learning rate.\n",
    "\n",
    "In practice better optimisers such as the adam optimiser will vary their learnig rate themselves and produce great results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1500 [..............................] - ETA: 39s - loss: 2.3550 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0489s). Check your callbacks.\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2271 - accuracy: 0.9327 - val_loss: 0.1283 - val_accuracy: 0.9601\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0938 - accuracy: 0.9719 - val_loss: 0.1002 - val_accuracy: 0.9693\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0612 - accuracy: 0.9805 - val_loss: 0.0852 - val_accuracy: 0.9758\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0456 - accuracy: 0.9855 - val_loss: 0.0803 - val_accuracy: 0.9783\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.0902 - val_accuracy: 0.9763\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.0872 - val_accuracy: 0.9772\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 0.0800 - val_accuracy: 0.9786\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0198 - accuracy: 0.9933 - val_loss: 0.0992 - val_accuracy: 0.9777\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 0.0985 - val_accuracy: 0.9778\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.0921 - val_accuracy: 0.9797\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.0961 - val_accuracy: 0.9797\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.1019 - val_accuracy: 0.9807\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.1142 - val_accuracy: 0.9782\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.1045 - val_accuracy: 0.9803\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0121 - accuracy: 0.9959 - val_loss: 0.1173 - val_accuracy: 0.9786\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0105 - accuracy: 0.9970 - val_loss: 0.1188 - val_accuracy: 0.9787\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.1276 - val_accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "adam_model = Sequential([\n",
    "             Flatten(input_shape=[rows, cols]),\n",
    "             Dense(300, activation='relu'),\n",
    "             Dense(100, activation='relu'),\n",
    "             Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "adam_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#                   optimizer=SGD(lr=1e-3), \n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "adam_root_logdir = os.path.join(os.curdir, \"adam_logs\")\n",
    "adam_run_logdir = get_run_logdir(adam_root_logdir)\n",
    "\n",
    "callbacks = [ModelCheckpoint(\"adam_model.h5\"),\n",
    "             EarlyStopping(patience=10, restore_best_weights=True),\n",
    "             TensorBoard(callback_run_logdir),\n",
    "#              IncreasingLearningRate(factor=1.005)\n",
    "            ]\n",
    "\n",
    "adam_trained_model = adam_model.fit(X_train, y_train,\n",
    "                                  epochs=100,\n",
    "                                  validation_data=(X_val, y_val),\n",
    "                                  callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
